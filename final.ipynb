{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  ✨ **Quora Question Pairs Prediction - NLP Final Project** ✨ \n\n\n---\n**Marah Habashi** - 211668751\n\n**Celine Karam** - 314658428\n\n\n---","metadata":{"execution":{"iopub.status.busy":"2023-06-10T11:54:38.775766Z","iopub.execute_input":"2023-06-10T11:54:38.776158Z","iopub.status.idle":"2023-06-10T11:54:38.783872Z","shell.execute_reply.started":"2023-06-10T11:54:38.776128Z","shell.execute_reply":"2023-06-10T11:54:38.782644Z"}}},{"cell_type":"markdown","source":"# **Importing Required Libraries:**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gensim\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,precision_score, recall_score,classification_report\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense,Bidirectional\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\nfrom bs4 import BeautifulSoup\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:21:46.402075Z","iopub.execute_input":"2023-06-10T21:21:46.403065Z","iopub.status.idle":"2023-06-10T21:21:46.410048Z","shell.execute_reply.started":"2023-06-10T21:21:46.403032Z","shell.execute_reply":"2023-06-10T21:21:46.408640Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"#  **Importing The Data:**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/quora-question-pairs/train.csv.zip')","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:21:46.411920Z","iopub.execute_input":"2023-06-10T21:21:46.412254Z","iopub.status.idle":"2023-06-10T21:21:47.753926Z","shell.execute_reply.started":"2023-06-10T21:21:46.412220Z","shell.execute_reply":"2023-06-10T21:21:47.752605Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"print(f'\\033[1m_______________________________ Shape of the data: {df.shape} __________________________________\\033[0m')\nprint(\"_____________________________________________data________________________________________________\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:21:47.755493Z","iopub.execute_input":"2023-06-10T21:21:47.756474Z","iopub.status.idle":"2023-06-10T21:21:47.769396Z","shell.execute_reply.started":"2023-06-10T21:21:47.756444Z","shell.execute_reply":"2023-06-10T21:21:47.768046Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"\u001b[1m_______________________________ Shape of the data: (404290, 6) __________________________________\u001b[0m\n_____________________________________________data________________________________________________\n","output_type":"stream"},{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"   id  qid1  qid2                                          question1  \\\n0   0     1     2  What is the step by step guide to invest in sh...   \n1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n2   2     5     6  How can I increase the speed of my internet co...   \n3   3     7     8  Why am I mentally very lonely? How can I solve...   \n4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n\n                                           question2  is_duplicate  \n0  What is the step by step guide to invest in sh...             0  \n1  What would happen if the Indian government sto...             0  \n2  How can Internet speed be increased by hacking...             0  \n3  Find the remainder when [math]23^{24}[/math] i...             0  \n4            Which fish would survive in salt water?             0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n      <td>What would happen if the Indian government sto...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>5</td>\n      <td>6</td>\n      <td>How can I increase the speed of my internet co...</td>\n      <td>How can Internet speed be increased by hacking...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>7</td>\n      <td>8</td>\n      <td>Why am I mentally very lonely? How can I solve...</td>\n      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>9</td>\n      <td>10</td>\n      <td>Which one dissolve in water quikly sugar, salt...</td>\n      <td>Which fish would survive in salt water?</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Data Isights:**","metadata":{}},{"cell_type":"code","source":"new_df = df.sample(30000,random_state=2)\nnew_df['is_duplicate'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:21:47.770748Z","iopub.execute_input":"2023-06-10T21:21:47.771639Z","iopub.status.idle":"2023-06-10T21:21:47.808151Z","shell.execute_reply.started":"2023-06-10T21:21:47.771607Z","shell.execute_reply":"2023-06-10T21:21:47.806504Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"0    19013\n1    10987\nName: is_duplicate, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"new_df[['question1','question2','is_duplicate']].iloc[4]","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:21:47.811284Z","iopub.execute_input":"2023-06-10T21:21:47.811924Z","iopub.status.idle":"2023-06-10T21:21:47.822667Z","shell.execute_reply.started":"2023-06-10T21:21:47.811895Z","shell.execute_reply":"2023-06-10T21:21:47.821652Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"question1                     Consequences of Bhopal gas tragedy?\nquestion2       What was the reason behind the Bhopal gas trag...\nis_duplicate                                                    0\nName: 151235, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Preprocessing Step:**","metadata":{}},{"cell_type":"markdown","source":"**The preprocess(q) function performs several text preprocessing steps on the input text q. Let's go through each step:**\n* Lowercasing and Stripping\n* Special Character Replacement\n* Removing '[math]' Pattern\n* Number Representation\n* Decontracting Words\n* HTML Tag Removal\n* Punctuation Removal\n\n**Finally, the preprocessed text q is returned by the function.**\n\n**Overall, the preprocess() function aims to clean and normalize the input text by removing special characters, standardizing numbers, expanding contractions, removing HTML tags, and eliminating punctuation. These preprocessing steps help in preparing the text data for further analysis or natural language processing tasks.**","metadata":{}},{"cell_type":"code","source":"def preprocess(q):\n    \n    q = str(q).lower().strip()\n    \n    # Replace certain special characters with their string equivalents\n    q = q.replace('%', ' percent')\n    q = q.replace('$', ' dollar ')\n    q = q.replace('₹', ' rupee ')\n    q = q.replace('€', ' euro ')\n    q = q.replace('@', ' at ')\n    \n    # The pattern '[math]' appears around 900 times in the whole dataset.\n    q = q.replace('[math]', '')\n    \n    # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n    q = q.replace(',000,000,000 ', 'b ')\n    q = q.replace(',000,000 ', 'm ')\n    q = q.replace(',000 ', 'k ')\n    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n    \n    # Decontracting words\n    # https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n    # https://stackoverflow.com/a/19794953\n    contractions = { \n    \"ain't\": \"am not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"can not\",\n    \"can't've\": \"can not have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"i'd\": \"i would\",\n    \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\",\n    \"i'll've\": \"i will have\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so as\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n    }\n\n    q_decontracted = []\n\n    for word in q.split():\n        if word in contractions:\n            word = contractions[word]\n\n        q_decontracted.append(word)\n\n    q = ' '.join(q_decontracted)\n    q = q.replace(\"'ve\", \" have\")\n    q = q.replace(\"n't\", \" not\")\n    q = q.replace(\"'re\", \" are\")\n    q = q.replace(\"'ll\", \" will\")\n    \n    # Removing HTML tags\n    q = BeautifulSoup(q)\n    q = q.get_text()\n    \n    # Remove punctuations\n    pattern = re.compile('\\W')\n    q = re.sub(pattern, ' ', q).strip()\n\n    \n    return q\n    ","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:21:47.824375Z","iopub.execute_input":"2023-06-10T21:21:47.824669Z","iopub.status.idle":"2023-06-10T21:21:47.843006Z","shell.execute_reply.started":"2023-06-10T21:21:47.824644Z","shell.execute_reply":"2023-06-10T21:21:47.841399Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"preprocess(\"I've already! wasn't <b>done</b>?\")","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:21:47.844630Z","iopub.execute_input":"2023-06-10T21:21:47.844995Z","iopub.status.idle":"2023-06-10T21:21:47.861833Z","shell.execute_reply.started":"2023-06-10T21:21:47.844965Z","shell.execute_reply":"2023-06-10T21:21:47.860489Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"'i have already  was not done'"},"metadata":{}}]},{"cell_type":"code","source":"new_df['question1'] = new_df['question1'].apply(preprocess)\nnew_df['question2'] = new_df['question2'].apply(preprocess)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:21:47.863576Z","iopub.execute_input":"2023-06-10T21:21:47.863994Z","iopub.status.idle":"2023-06-10T21:22:00.947562Z","shell.execute_reply.started":"2023-06-10T21:21:47.863965Z","shell.execute_reply":"2023-06-10T21:22:00.946487Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"new_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:00.948598Z","iopub.execute_input":"2023-06-10T21:22:00.949322Z","iopub.status.idle":"2023-06-10T21:22:00.961702Z","shell.execute_reply.started":"2023-06-10T21:22:00.949291Z","shell.execute_reply":"2023-06-10T21:22:00.960283Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"            id    qid1    qid2  \\\n398782  398782  496695  532029   \n115086  115086  187729  187730   \n327711  327711  454161  454162   \n367788  367788  498109  491396   \n151235  151235  237843   50930   \n\n                                                question1  \\\n398782  what is the best marketing automation tool for...   \n115086   i am poor but i want to invest  what should i do   \n327711  i am from india and live abroad  i met a guy f...   \n367788  why do so many people in the u s  hate the sou...   \n151235                 consequences of bhopal gas tragedy   \n\n                                                question2  is_duplicate  \n398782  what is the best marketing automation tool for...             1  \n115086  i am quite poor and i want to be very rich  wh...             0  \n327711  t i e t to thapar university to thapar univers...             0  \n367788  my boyfriend doesnt feel guilty when he hurts ...             0  \n151235  what was the reason behind the bhopal gas tragedy             0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>398782</th>\n      <td>398782</td>\n      <td>496695</td>\n      <td>532029</td>\n      <td>what is the best marketing automation tool for...</td>\n      <td>what is the best marketing automation tool for...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>115086</th>\n      <td>115086</td>\n      <td>187729</td>\n      <td>187730</td>\n      <td>i am poor but i want to invest  what should i do</td>\n      <td>i am quite poor and i want to be very rich  wh...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>327711</th>\n      <td>327711</td>\n      <td>454161</td>\n      <td>454162</td>\n      <td>i am from india and live abroad  i met a guy f...</td>\n      <td>t i e t to thapar university to thapar univers...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>367788</th>\n      <td>367788</td>\n      <td>498109</td>\n      <td>491396</td>\n      <td>why do so many people in the u s  hate the sou...</td>\n      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>151235</th>\n      <td>151235</td>\n      <td>237843</td>\n      <td>50930</td>\n      <td>consequences of bhopal gas tragedy</td>\n      <td>what was the reason behind the bhopal gas tragedy</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Calculating Some Useful Features:**","metadata":{}},{"cell_type":"markdown","source":"Following lines of code add two new columns to the DataFrames new_df, respectively, containing the lengths of the values in the question1 and question2 columns.","metadata":{}},{"cell_type":"code","source":"new_df['q1_len'] = new_df['question1'].str.len() \nnew_df['q2_len'] = new_df['question2'].str.len()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:00.963318Z","iopub.execute_input":"2023-06-10T21:22:00.963736Z","iopub.status.idle":"2023-06-10T21:22:01.001805Z","shell.execute_reply.started":"2023-06-10T21:22:00.963691Z","shell.execute_reply":"2023-06-10T21:22:01.000430Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"These lines of code add two new columns to the DataFrames new_df , respectively, containing the number of words in each sentence of the question1 and question2 columns. The number of words is calculated by splitting the sentences by spaces and counting the resulting words.","metadata":{}},{"cell_type":"code","source":"new_df['q1_num_words'] = new_df['question1'].apply(lambda row: len(row.split(\" \")))\nnew_df['q2_num_words'] = new_df['question2'].apply(lambda row: len(row.split(\" \")))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:01.006024Z","iopub.execute_input":"2023-06-10T21:22:01.006339Z","iopub.status.idle":"2023-06-10T21:22:01.075657Z","shell.execute_reply.started":"2023-06-10T21:22:01.006315Z","shell.execute_reply":"2023-06-10T21:22:01.074475Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"Defines a function called common_words that takes a row as input. Inside the function, it splits the sentences in the question1 and question2 columns of the row into words using the space character as a delimiter. It then converts the words to lowercase and removes any leading or trailing whitespace. Next, it creates sets w1 and w2 from these processed words. Finally, it returns the length of the intersection (&) of w1 and w2, representing the number of common words between the two sentences. This function can be used to calculate the number of common words for each row in a DataFrame.","metadata":{}},{"cell_type":"code","source":"def common_words(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return len(w1 & w2)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:01.076851Z","iopub.execute_input":"2023-06-10T21:22:01.077090Z","iopub.status.idle":"2023-06-10T21:22:01.083772Z","shell.execute_reply.started":"2023-06-10T21:22:01.077069Z","shell.execute_reply":"2023-06-10T21:22:01.082271Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"new_df['word_common'] = new_df.apply(common_words, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:01.085301Z","iopub.execute_input":"2023-06-10T21:22:01.085642Z","iopub.status.idle":"2023-06-10T21:22:01.644277Z","shell.execute_reply.started":"2023-06-10T21:22:01.085614Z","shell.execute_reply":"2023-06-10T21:22:01.643518Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"Defines a function called total_words that takes a row as input. It splits the values in the 'question1' and 'question2' columns of the row into individual words and converts them to lowercase after removing leading and trailing spaces. It then creates sets of unique words for both 'question1' and 'question2'. Finally, it returns the total count of unique words in both questions combined by adding the lengths of the two sets.","metadata":{}},{"cell_type":"code","source":"def total_words(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return (len(w1) + len(w2))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:01.645605Z","iopub.execute_input":"2023-06-10T21:22:01.646102Z","iopub.status.idle":"2023-06-10T21:22:01.651664Z","shell.execute_reply.started":"2023-06-10T21:22:01.646073Z","shell.execute_reply":"2023-06-10T21:22:01.650478Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"new_df['word_total'] = new_df.apply(total_words, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:01.652960Z","iopub.execute_input":"2023-06-10T21:22:01.653263Z","iopub.status.idle":"2023-06-10T21:22:02.194526Z","shell.execute_reply.started":"2023-06-10T21:22:01.653234Z","shell.execute_reply":"2023-06-10T21:22:02.193349Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"new_df['word_share'] = round(new_df['word_common']/new_df['word_total'],2)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:02.196033Z","iopub.execute_input":"2023-06-10T21:22:02.196404Z","iopub.status.idle":"2023-06-10T21:22:02.204089Z","shell.execute_reply.started":"2023-06-10T21:22:02.196377Z","shell.execute_reply":"2023-06-10T21:22:02.202473Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"# **Preprocessings Function for removing stop words , replacing cotractions and tokenization:**","metadata":{}},{"cell_type":"markdown","source":"**Defines a function called fetch_token_features that takes a row as input. It performs several operations to extract token-based features from the 'question1' and 'question2' columns of the row. Here's a summary of what the code does:**\n\nIt imports the stopwords from the NLTK (Natural Language Toolkit) corpus for the English language.\n\nIt initializes a list token_features with eight elements, all initially set to 0.0.\n\nThe 'question1' and 'question2' values from the row are assigned to variables q1 and q2, respectively.\n\nIt splits q1 and q2 into individual tokens (words).\n\nIf either q1 or q2 has no tokens (empty), it returns the token_features list.\n\nIt filters out the stopwords from q1 and q2, creating sets of non-stopword words.\n\nIt also creates sets of stopwords from q1 and q2.\n\nIt calculates the count of common non-stopword words, common stopwords, and common tokens between q1 and q2.\n\nThe token-based features are computed and stored in the token_features list as follows:\n\n**Index 0**: Ratio of common non-stopword words to the minimum length of q1_words and q2_words.\n\n**Index 1**: Ratio of common non-stopword words to the maximum length of q1_words and q2_words.\n\n**Index 2**: Ratio of common stopwords to the minimum length of q1_stops and q2_stops.\n\n**Index 3**: Ratio of common stopwords to the maximum length of q1_stops and q2_stops.\n\n**Index 4**: Ratio of common tokens to the minimum length of q1_tokens and q2_tokens.\n\n**Index 5**: Ratio of common tokens to the maximum length of q1_tokens and q2_tokens.\n\n**Index 6**: Indicator (1 or 0) whether the last word of q1 is the same as the last word of q2.\n\n**Index 7**: Indicator (1 or 0) whether the first word of q1 is the same as the first word of q2.\n\nFinally, the token_features list is returned as the output.","metadata":{}},{"cell_type":"code","source":"# Advanced Features\nfrom nltk.corpus import stopwords\n\ndef fetch_token_features(row):\n    \n    q1 = row['question1']\n    q2 = row['question2']\n    \n    SAFE_DIV = 0.0001 \n\n    STOP_WORDS = stopwords.words(\"english\")\n    \n    token_features = [0.0]*8\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n\n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    #Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    return token_features\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:02.205955Z","iopub.execute_input":"2023-06-10T21:22:02.206475Z","iopub.status.idle":"2023-06-10T21:22:02.220118Z","shell.execute_reply.started":"2023-06-10T21:22:02.206441Z","shell.execute_reply":"2023-06-10T21:22:02.218302Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"token_features = new_df.apply(fetch_token_features, axis=1)\n\nnew_df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\nnew_df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\nnew_df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\nnew_df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\nnew_df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\nnew_df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\nnew_df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\nnew_df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:02.222168Z","iopub.execute_input":"2023-06-10T21:22:02.222637Z","iopub.status.idle":"2023-06-10T21:22:08.429736Z","shell.execute_reply.started":"2023-06-10T21:22:02.222603Z","shell.execute_reply":"2023-06-10T21:22:08.428502Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"!pip install distance","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:08.431229Z","iopub.execute_input":"2023-06-10T21:22:08.431607Z","iopub.status.idle":"2023-06-10T21:22:17.223192Z","shell.execute_reply.started":"2023-06-10T21:22:08.431572Z","shell.execute_reply":"2023-06-10T21:22:17.221902Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"Requirement already satisfied: distance in /opt/conda/lib/python3.10/site-packages (0.1.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import distance\n\ndef fetch_length_features(row):\n    \n    q1 = row['question1']\n    q2 = row['question2']\n    \n    length_features = [0.0]*3\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return length_features\n    \n    # Absolute length features\n    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    #Average Token Length of both Questions\n    length_features[1] = (len(q1_tokens) + len(q2_tokens))/2\n    \n    strs = list(distance.lcsubstrings(q1, q2))\n    length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n    \n    return length_features\n    ","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:17.224717Z","iopub.execute_input":"2023-06-10T21:22:17.225062Z","iopub.status.idle":"2023-06-10T21:22:17.232586Z","shell.execute_reply.started":"2023-06-10T21:22:17.225031Z","shell.execute_reply":"2023-06-10T21:22:17.231478Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"length_features = new_df.apply(fetch_length_features, axis=1)\n\nnew_df['abs_len_diff'] = list(map(lambda x: x[0], length_features))\nnew_df['mean_len'] = list(map(lambda x: x[1], length_features))\nnew_df['longest_substr_ratio'] = list(map(lambda x: x[2], length_features))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:17.233663Z","iopub.execute_input":"2023-06-10T21:22:17.234023Z","iopub.status.idle":"2023-06-10T21:22:41.864404Z","shell.execute_reply.started":"2023-06-10T21:22:17.233989Z","shell.execute_reply":"2023-06-10T21:22:41.863470Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"# Fuzzy Features\nfrom fuzzywuzzy import fuzz\n\ndef fetch_fuzzy_features(row):\n    \n    q1 = row['question1']\n    q2 = row['question2']\n    \n    fuzzy_features = [0.0]*4\n    \n    # fuzz_ratio\n    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n\n    # fuzz_partial_ratio\n    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n\n    # token_sort_ratio\n    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n\n    # token_set_ratio\n    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n\n    return fuzzy_features","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:41.865499Z","iopub.execute_input":"2023-06-10T21:22:41.865751Z","iopub.status.idle":"2023-06-10T21:22:41.871262Z","shell.execute_reply.started":"2023-06-10T21:22:41.865728Z","shell.execute_reply":"2023-06-10T21:22:41.870428Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"fuzzy_features = new_df.apply(fetch_fuzzy_features, axis=1)\n\n# Creating new feature columns for fuzzy features\nnew_df['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))\nnew_df['fuzz_partial_ratio'] = list(map(lambda x: x[1], fuzzy_features))\nnew_df['token_sort_ratio'] = list(map(lambda x: x[2], fuzzy_features))\nnew_df['token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:41.872435Z","iopub.execute_input":"2023-06-10T21:22:41.872676Z","iopub.status.idle":"2023-06-10T21:22:45.590393Z","shell.execute_reply.started":"2023-06-10T21:22:41.872649Z","shell.execute_reply":"2023-06-10T21:22:45.588684Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"print(new_df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:45.591804Z","iopub.execute_input":"2023-06-10T21:22:45.592115Z","iopub.status.idle":"2023-06-10T21:22:45.597827Z","shell.execute_reply.started":"2023-06-10T21:22:45.592091Z","shell.execute_reply":"2023-06-10T21:22:45.596745Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"(30000, 28)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Subsetting Required Data For Modeling:**","metadata":{}},{"cell_type":"code","source":"ques_df = new_df[['question1','question2']]\nques_df.head(4)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:45.598920Z","iopub.execute_input":"2023-06-10T21:22:45.599180Z","iopub.status.idle":"2023-06-10T21:22:45.618892Z","shell.execute_reply.started":"2023-06-10T21:22:45.599157Z","shell.execute_reply":"2023-06-10T21:22:45.617294Z"},"trusted":true},"execution_count":90,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"                                                question1  \\\n398782  what is the best marketing automation tool for...   \n115086   i am poor but i want to invest  what should i do   \n327711  i am from india and live abroad  i met a guy f...   \n367788  why do so many people in the u s  hate the sou...   \n\n                                                question2  \n398782  what is the best marketing automation tool for...  \n115086  i am quite poor and i want to be very rich  wh...  \n327711  t i e t to thapar university to thapar univers...  \n367788  my boyfriend doesnt feel guilty when he hurts ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question1</th>\n      <th>question2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>398782</th>\n      <td>what is the best marketing automation tool for...</td>\n      <td>what is the best marketing automation tool for...</td>\n    </tr>\n    <tr>\n      <th>115086</th>\n      <td>i am poor but i want to invest  what should i do</td>\n      <td>i am quite poor and i want to be very rich  wh...</td>\n    </tr>\n    <tr>\n      <th>327711</th>\n      <td>i am from india and live abroad  i met a guy f...</td>\n      <td>t i e t to thapar university to thapar univers...</td>\n    </tr>\n    <tr>\n      <th>367788</th>\n      <td>why do so many people in the u s  hate the sou...</td>\n      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Preparing Data Countvectorizer To Get Bag of words:**","metadata":{}},{"cell_type":"code","source":"questions = list(ques_df['question1']) + list(ques_df['question2'])","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:45.620260Z","iopub.execute_input":"2023-06-10T21:22:45.620600Z","iopub.status.idle":"2023-06-10T21:22:45.640282Z","shell.execute_reply.started":"2023-06-10T21:22:45.620572Z","shell.execute_reply":"2023-06-10T21:22:45.639193Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"questions[:10]","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:45.641639Z","iopub.execute_input":"2023-06-10T21:22:45.642086Z","iopub.status.idle":"2023-06-10T21:22:45.653642Z","shell.execute_reply.started":"2023-06-10T21:22:45.642054Z","shell.execute_reply":"2023-06-10T21:22:45.652760Z"},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"['what is the best marketing automation tool for small and mid size companies',\n 'i am poor but i want to invest  what should i do',\n 'i am from india and live abroad  i met a guy from france in a party i want to date him  how do i do that',\n 'why do so many people in the u s  hate the southern states',\n 'consequences of bhopal gas tragedy',\n 'i killed a snake on a friday  there is a belief that when you kill a snake on a friday it will certainly take revenge  will i be killed',\n 'is the royal family a net gain or a net loss to the british taxpayer',\n 'if a huge asteroid was about to hit earth in x year  would we be able to find survival solutions in due time',\n 'what would happen if a woman took viagra',\n 'how could i improve my love to my girlfriend']"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Bag of Words:**","metadata":{}},{"cell_type":"markdown","source":"CountVectorizer from the sklearn.feature_extraction.text module to convert a collection of text documents (questions) into a matrix representation of word occurrences.\n\nHere's a summary of what the code does:\n\nIt imports the CountVectorizer class from the sklearn.feature_extraction.text module. It initializes an instance of CountVectorizer named vectorizer with the following parameters: max_features=1000: Limits the number of features (words) to the top 1000 most frequent words based on their occurrence in the training data. stop_words='english': Specifies that common English stopwords should be excluded from the vocabulary. It calls the fit_transform method of vectorizer on the questions data (presumably a list or array-like object containing the training questions) to learn the vocabulary and transform the questions into a matrix representation. fit_transform learns the vocabulary from the training data and returns a sparse matrix representation of the questions, where each row corresponds to a question, and each column represents a word in the vocabulary. The resulting matrix X_train is a sparse matrix with dimensions (number of questions, number of unique words in the vocabulary). It calls the transform method of vectorizer on the questions_t data (presumably a list or array-like object containing the test questions) to transform the test questions into the same matrix representation as the training data. transform applies the learned vocabulary from the training data to the test data and returns a sparse matrix representation. The resulting matrix X_test is a sparse matrix with the same dimensions as X_train. It calls the toarray method on X_train to convert the sparse matrix representation of the training data into a dense matrix. toarray converts the sparse matrix to a regular NumPy array. The resulting matrix bag_of_word is a dense matrix with dimensions (number of questions, number of unique words in the vocabulary), where each element represents the count of a word in a specific question.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# Initialize the CountVectorizer\nvectorizer = CountVectorizer(max_features=1000,stop_words='english')\n\n# Fit the vectorizer on the questions to learn the vocabulary\nX_train = vectorizer.fit_transform(questions)\n\n#X_test = vectorizer.transform(questions_t)\n\n# Convert the bag-of-words representation to a dense matrix\nbag_of_word = X_train.toarray()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:45.654581Z","iopub.execute_input":"2023-06-10T21:22:45.654929Z","iopub.status.idle":"2023-06-10T21:22:46.408972Z","shell.execute_reply.started":"2023-06-10T21:22:45.654896Z","shell.execute_reply":"2023-06-10T21:22:46.407734Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:46.416052Z","iopub.execute_input":"2023-06-10T21:22:46.416369Z","iopub.status.idle":"2023-06-10T21:22:46.422532Z","shell.execute_reply.started":"2023-06-10T21:22:46.416344Z","shell.execute_reply":"2023-06-10T21:22:46.421529Z"},"trusted":true},"execution_count":94,"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"<60000x1000 sparse matrix of type '<class 'numpy.int64'>'\n\twith 174767 stored elements in Compressed Sparse Row format>"},"metadata":{}}]},{"cell_type":"code","source":"(X_train !=0).sum()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:46.423598Z","iopub.execute_input":"2023-06-10T21:22:46.423888Z","iopub.status.idle":"2023-06-10T21:22:46.442036Z","shell.execute_reply.started":"2023-06-10T21:22:46.423863Z","shell.execute_reply":"2023-06-10T21:22:46.440904Z"},"trusted":true},"execution_count":95,"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"174767"},"metadata":{}}]},{"cell_type":"code","source":"print(f'What percentage of values are non zero : {(X_train !=0).sum()/np.prod(X_train.shape)}')","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:46.443461Z","iopub.execute_input":"2023-06-10T21:22:46.443791Z","iopub.status.idle":"2023-06-10T21:22:46.450352Z","shell.execute_reply.started":"2023-06-10T21:22:46.443760Z","shell.execute_reply":"2023-06-10T21:22:46.449694Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"What percentage of values are non zero : 0.0029127833333333335\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **TFIDF Vectors:**","metadata":{}},{"cell_type":"markdown","source":"TfidfVectorizer to transform a collection of text questions into a matrix of TF-IDF values, and then converts the sparse matrix representation of the TF-IDF data into a dense matrix.","metadata":{}},{"cell_type":"code","source":"# Initialize the TfidfVectorizer and fit on the questions to learn the vocabulary\nvectorizer = TfidfVectorizer(max_features=1000,stop_words='english')\ntfidf = vectorizer.fit_transform(questions)\n# Convert the TF-IDF representation to a dense matrix\ntfidf = tfidf.toarray()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:46.451367Z","iopub.execute_input":"2023-06-10T21:22:46.451599Z","iopub.status.idle":"2023-06-10T21:22:47.217047Z","shell.execute_reply.started":"2023-06-10T21:22:46.451578Z","shell.execute_reply":"2023-06-10T21:22:47.215345Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"Here we retrieves the vocabulary learned by the TfidfVectorizer, prints a subset of the vocabulary, and prints a subset of the TF-IDF representation of the questions.","metadata":{}},{"cell_type":"code","source":"# Retrieve the vocabulary\nvocabulary = vectorizer.get_feature_names_out()\n# Print a subset of the vocabulary (optional)\n# Print a subset of the vocabulary (optional)\nmax_vocabulary_display = 100\nprint(\"Vocabulary (subset):\")\nprint(vocabulary[:max_vocabulary_display])\n\n# Print a subset of the TF-IDF representation (optional)\nmax_tfidf_display = 100\nprint(\"TF-IDF representation (subset):\")\nprint(tfidf[:max_tfidf_display])","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:47.218507Z","iopub.execute_input":"2023-06-10T21:22:47.218854Z","iopub.status.idle":"2023-06-10T21:22:47.228123Z","shell.execute_reply.started":"2023-06-10T21:22:47.218824Z","shell.execute_reply":"2023-06-10T21:22:47.226734Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"Vocabulary (subset):\n['10' '100' '11' '12' '12th' '13' '15' '16' '18' '1k' '20' '2014' '2015'\n '2016' '2017' '24' '2k' '30' '50' '500' 'able' 'abroad' 'access'\n 'account' 'accounts' 'acne' 'act' 'actor' 'actually' 'add' 'address'\n 'admission' 'advanced' 'advantages' 'advice' 'affect' 'age' 'ago' 'air'\n 'alcohol' 'allowed' 'amazon' 'america' 'american' 'americans' 'ancient'\n 'android' 'animals' 'answer' 'answers' 'anxiety' 'app' 'apple'\n 'application' 'applications' 'apply' 'approach' 'apps' 'area' 'army'\n 'art' 'ask' 'asked' 'attack' 'attractive' 'australia' 'available'\n 'average' 'avoid' 'away' 'bad' 'balance' 'ban' 'bangalore' 'bank'\n 'banning' 'based' 'basic' 'battle' 'beautiful' 'believe' 'belly'\n 'benefits' 'best' 'better' 'big' 'biggest' 'birth' 'birthday' 'black'\n 'block' 'blocked' 'blog' 'blood' 'blowing' 'blue' 'board' 'body'\n 'bollywood' 'book']\nTF-IDF representation (subset):\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Applying word2vec as well:**","metadata":{}},{"cell_type":"code","source":"import gensim","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:47.230005Z","iopub.execute_input":"2023-06-10T21:22:47.230401Z","iopub.status.idle":"2023-06-10T21:22:47.243407Z","shell.execute_reply.started":"2023-06-10T21:22:47.230366Z","shell.execute_reply":"2023-06-10T21:22:47.242184Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"questions = list(ques_df['question1']) + list(ques_df['question2'])","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:47.244865Z","iopub.execute_input":"2023-06-10T21:22:47.246135Z","iopub.status.idle":"2023-06-10T21:22:47.267008Z","shell.execute_reply.started":"2023-06-10T21:22:47.246080Z","shell.execute_reply":"2023-06-10T21:22:47.265955Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"The simple_preprocess function to preprocess each sentence in the questions list and stores the processed sentences in the ques_sent list.","metadata":{}},{"cell_type":"code","source":"ques_sent = []\nfor sentence in questions:\n    ques_sent.append(gensim.utils.simple_preprocess(sentence))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:47.268367Z","iopub.execute_input":"2023-06-10T21:22:47.268730Z","iopub.status.idle":"2023-06-10T21:22:48.140657Z","shell.execute_reply.started":"2023-06-10T21:22:47.268697Z","shell.execute_reply":"2023-06-10T21:22:48.139282Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":"In following lines of code we initializes a Word2Vec model object with specific parameters:\n\nwindow=2 sets the maximum distance between the target word and its context words to 2. min_count=3 sets the minimum frequency count of words to 3. Words that occur less frequently than this will be ignored. sg=1 indicates the use of the Skip-gram algorithm. Alternative value of sg=0 would use the Continuous Bag of Words (CBOW) algorithm. vector_size=100 sets the dimensionality of the word vectors to 100. It builds the vocabulary of the Word2Vec model by calling the build_vocab method and passing the preprocessed sentences (ques_sent).\n\nIt trains the Word2Vec model by calling the train method and passing the preprocessed sentences (ques_sent) as the training corpus.\n\ncorpus_iterable specifies the input corpus as an iterable of sentences. total_examples specifies the total number of sentences in the corpus. epochs specifies the number of training epochs (passes over the corpus) to perform. After training, it retrieves the word vectors from the trained model and stores them in the w2v dictionary.\n\nmodel.wv.index_to_key retrieves the vocabulary words as a list. model.wv.vectors.round(3) retrieves the corresponding word vectors, rounded to 3 decimal places. dict(zip(...)) creates a dictionary mapping each word to its vector.","metadata":{}},{"cell_type":"code","source":"model  = gensim.models.Word2Vec(window=2,min_count=3,sg=1,vector_size=100)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:48.142523Z","iopub.execute_input":"2023-06-10T21:22:48.142979Z","iopub.status.idle":"2023-06-10T21:22:48.150248Z","shell.execute_reply.started":"2023-06-10T21:22:48.142940Z","shell.execute_reply":"2023-06-10T21:22:48.148881Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"model.build_vocab(ques_sent)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:48.151732Z","iopub.execute_input":"2023-06-10T21:22:48.152094Z","iopub.status.idle":"2023-06-10T21:22:48.513866Z","shell.execute_reply.started":"2023-06-10T21:22:48.152061Z","shell.execute_reply":"2023-06-10T21:22:48.512803Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"model.train(corpus_iterable=ques_sent,total_examples= model.corpus_count, epochs=model.epochs)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:48.515036Z","iopub.execute_input":"2023-06-10T21:22:48.515752Z","iopub.status.idle":"2023-06-10T21:22:51.724774Z","shell.execute_reply.started":"2023-06-10T21:22:48.515700Z","shell.execute_reply":"2023-06-10T21:22:51.723876Z"},"trusted":true},"execution_count":104,"outputs":[{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"(2174565, 3150050)"},"metadata":{}}]},{"cell_type":"code","source":"w2v = dict(zip(model.wv.index_to_key, (model.wv.vectors.round(3))))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:51.726316Z","iopub.execute_input":"2023-06-10T21:22:51.726666Z","iopub.status.idle":"2023-06-10T21:22:51.740120Z","shell.execute_reply.started":"2023-06-10T21:22:51.726624Z","shell.execute_reply":"2023-06-10T21:22:51.738576Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"w2v['what']","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:51.741803Z","iopub.execute_input":"2023-06-10T21:22:51.743110Z","iopub.status.idle":"2023-06-10T21:22:51.752163Z","shell.execute_reply.started":"2023-06-10T21:22:51.743057Z","shell.execute_reply":"2023-06-10T21:22:51.751043Z"},"trusted":true},"execution_count":106,"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"array([-6.200e-01,  1.020e-01,  2.910e-01,  2.430e-01,  3.600e-01,\n       -4.650e-01,  4.100e-01,  5.140e-01, -3.900e-02, -4.040e-01,\n       -2.780e-01,  1.800e-02, -1.000e-03,  2.740e-01,  3.080e-01,\n       -1.960e-01,  1.600e-02, -4.400e-02, -2.300e-01, -4.920e-01,\n        7.060e-01,  1.600e-01,  9.250e-01, -9.480e-01,  3.840e-01,\n        1.400e-02,  3.450e-01,  6.090e-01, -3.140e-01,  3.410e-01,\n        1.000e+00, -4.840e-01,  2.960e-01, -7.650e-01,  4.300e-02,\n        3.240e-01,  2.770e-01,  3.210e-01,  6.200e-02,  3.690e-01,\n        5.500e-02, -4.460e-01, -5.110e-01, -4.260e-01,  4.200e-02,\n        2.860e-01,  5.200e-02, -9.000e-02, -1.340e-01,  1.320e-01,\n        5.100e-02, -7.240e-01, -3.330e-01, -2.010e-01,  1.840e-01,\n        4.280e-01,  3.290e-01,  5.680e-01, -9.000e-03,  3.460e-01,\n       -3.770e-01,  2.700e-02, -2.800e-02, -1.320e-01, -1.330e-01,\n        6.150e-01,  7.640e-01, -2.300e-02, -3.380e-01,  1.297e+00,\n       -1.260e-01, -6.900e-02,  5.570e-01, -4.740e-01,  6.730e-01,\n       -3.900e-02, -4.260e-01, -1.850e-01, -1.100e-02, -2.200e-01,\n       -7.830e-01,  2.500e-02, -4.180e-01,  1.000e-01, -1.500e-01,\n       -3.610e-01, -1.600e-02, -3.080e-01,  1.950e-01,  1.620e-01,\n        2.840e-01, -1.560e-01,  1.050e-01, -3.840e-01,  1.460e-01,\n        3.300e-02,  9.260e-01, -9.500e-02, -5.600e-02, -1.730e-01],\n      dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"w2v['why']","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:51.753886Z","iopub.execute_input":"2023-06-10T21:22:51.754331Z","iopub.status.idle":"2023-06-10T21:22:51.767397Z","shell.execute_reply.started":"2023-06-10T21:22:51.754289Z","shell.execute_reply":"2023-06-10T21:22:51.766437Z"},"trusted":true},"execution_count":107,"outputs":[{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"array([-0.604,  0.876, -0.516, -0.1  , -0.055, -0.458, -0.212,  0.228,\n       -0.298, -0.809,  0.03 , -0.268, -0.098,  0.339,  0.547, -0.12 ,\n        0.22 , -0.552, -0.615, -0.787,  0.119, -0.155,  0.519,  0.013,\n        0.227,  0.172, -0.23 , -0.207, -0.089,  0.46 ,  1.215, -0.493,\n        0.309, -0.057, -0.104,  0.301,  0.084,  0.125, -0.081, -0.248,\n       -0.159, -0.264,  0.524, -0.031,  0.009, -0.637, -0.207, -0.099,\n        0.175,  0.118,  0.468, -0.516, -0.199, -0.097,  0.083,  0.47 ,\n        0.359,  0.134, -0.437,  0.193,  0.348,  0.248,  0.527,  0.377,\n        0.316,  0.178, -0.024, -0.157, -0.151,  0.355,  0.227,  0.056,\n        0.382,  0.188,  0.464,  0.378,  0.185,  0.322,  0.069,  0.131,\n       -0.032, -0.469, -0.104, -0.202,  0.384,  0.108,  0.255,  0.361,\n       -0.043,  0.066,  0.386,  0.26 ,  0.294,  0.013,  0.45 , -0.265,\n        0.794,  0.122,  0.088,  0.049], dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# **TF-IDF Vector For word2vec:**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:51.768780Z","iopub.execute_input":"2023-06-10T21:22:51.769299Z","iopub.status.idle":"2023-06-10T21:22:51.780039Z","shell.execute_reply.started":"2023-06-10T21:22:51.769265Z","shell.execute_reply":"2023-06-10T21:22:51.778461Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"tfidf1 = TfidfVectorizer()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:51.781767Z","iopub.execute_input":"2023-06-10T21:22:51.782406Z","iopub.status.idle":"2023-06-10T21:22:51.794290Z","shell.execute_reply.started":"2023-06-10T21:22:51.782368Z","shell.execute_reply":"2023-06-10T21:22:51.793049Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"tfidf1.fit_transform(questions)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:51.796333Z","iopub.execute_input":"2023-06-10T21:22:51.796740Z","iopub.status.idle":"2023-06-10T21:22:52.486049Z","shell.execute_reply.started":"2023-06-10T21:22:51.796704Z","shell.execute_reply":"2023-06-10T21:22:52.485118Z"},"trusted":true},"execution_count":110,"outputs":[{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"<60000x26200 sparse matrix of type '<class 'numpy.float64'>'\n\twith 606923 stored elements in Compressed Sparse Row format>"},"metadata":{}}]},{"cell_type":"code","source":"a = tfidf1.vocabulary_.items()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:52.487089Z","iopub.execute_input":"2023-06-10T21:22:52.487358Z","iopub.status.idle":"2023-06-10T21:22:52.492930Z","shell.execute_reply.started":"2023-06-10T21:22:52.487335Z","shell.execute_reply":"2023-06-10T21:22:52.491917Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"# **Creating a dictionary word2weight that maps each word in the vocabulary of a TF-IDF vectorizer (tfidf1) to its corresponding IDF (Inverse Document Frequency) weight:**","metadata":{}},{"cell_type":"code","source":"word2weight = [(w, round(tfidf1.idf_[i])) for w, i in tfidf1.vocabulary_.items()]","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:22:52.494096Z","iopub.execute_input":"2023-06-10T21:22:52.494350Z","iopub.status.idle":"2023-06-10T21:23:00.376915Z","shell.execute_reply.started":"2023-06-10T21:22:52.494329Z","shell.execute_reply":"2023-06-10T21:23:00.375409Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"word2weight = dict(word2weight)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:00.378676Z","iopub.execute_input":"2023-06-10T21:23:00.379016Z","iopub.status.idle":"2023-06-10T21:23:00.393700Z","shell.execute_reply.started":"2023-06-10T21:23:00.378987Z","shell.execute_reply":"2023-06-10T21:23:00.392589Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"model.wv.similar_by_word('pakistan',topn=15)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:00.395692Z","iopub.execute_input":"2023-06-10T21:23:00.396068Z","iopub.status.idle":"2023-06-10T21:23:00.409236Z","shell.execute_reply.started":"2023-06-10T21:23:00.396035Z","shell.execute_reply":"2023-06-10T21:23:00.408492Z"},"trusted":true},"execution_count":114,"outputs":[{"execution_count":114,"output_type":"execute_result","data":{"text/plain":"[('russia', 0.9117721915245056),\n ('china', 0.8754943013191223),\n ('strike', 0.8475841283798218),\n ('attack', 0.8412674069404602),\n ('declare', 0.8370988965034485),\n ('kashmir', 0.828040599822998),\n ('vietnam', 0.8251993060112),\n ('terrorists', 0.8244001269340515),\n ('israel', 0.8226666450500488),\n ('syria', 0.8189303278923035),\n ('defeat', 0.8165336847305298),\n ('declared', 0.8101333975791931),\n ('uri', 0.8079087734222412),\n ('invade', 0.8062509298324585),\n ('north', 0.8046808242797852)]"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Data splitting in train test and val sets also prepration for modeling:**","metadata":{}},{"cell_type":"markdown","source":"Defines a function document_vector that calculates a document vector based on a given document.\n\nHere's a summary of what the code does:\n\nIt checks the length of the input document:\n\nIf the document has no words (len(doc.split()) == 0) or has only one word (len(doc.split()) == 1), it returns a zero vector of shape (100). If the document has more than one word:\n\nIt initializes an empty list doc_vec to store the word vectors multiplied by their corresponding TF-IDF weights. It initializes a variable tfidf_weight_sum to store the sum of TF-IDF weights. It iterates over each word in the document. For each word that exists in both the word2vec model (w2v) and the TF-IDF weights (word2weight), it calculates the TF-IDF weight for the word based on its frequency in the document. It multiplies the word vector (w2v[word]) by the TF-IDF weight and appends the result to the doc_vec list. It accumulates the TF-IDF weight in the tfidf_weight_sum variable. After processing all the words in the document:\n\nIf no valid word vectors were found (len(doc_vec) == 0), it returns a zero vector of shape (100). Otherwise, it calculates the weighted average of the word vectors by summing them (np.sum(doc_vec, axis=0)) and dividing by the total TF-IDF weight (tfidf_weight_sum). The resulting document vector is rounded to 3 decimal places using np.round().","metadata":{}},{"cell_type":"code","source":"def document_vector(doc):\n    if len(doc.split()) == 0:\n        return np.zeros(shape=(100))\n    elif len(doc.split()) == 1:\n        return np.zeros(shape=(100))\n    else:\n#         doc = [word for word in doc.split() if word in model.wv.index_to_key]\n#         return np.mean(model.wv[doc],axis=0).round(2)\n        doc_vec = []\n        tfidf_weight_sum = 0\n        for word in doc.split():\n            if word in w2v.keys() and word in word2weight.keys():\n                tfidf_weight = word2weight[word]*doc.split().count(word)/len(doc.split())\n                product = (w2v[word]*tfidf_weight)\n                doc_vec.append(product)\n                tfidf_weight_sum = tfidf_weight_sum + tfidf_weight\n                #print(f\"weight of {word} : {word2weight[word]}\")\n                #print(f\"word vector of {word} : {w2v[word]}\")\n                #print(product)\\n\",\n        #print(doc_vec)\n        if len(doc_vec) == 0:\n            return np.round(np.sum(doc_vec,axis=0)/1,3)\n        else:\n            return np.round(np.sum(doc_vec,axis=0)/tfidf_weight_sum,3)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:00.410734Z","iopub.execute_input":"2023-06-10T21:23:00.411273Z","iopub.status.idle":"2023-06-10T21:23:00.419084Z","shell.execute_reply.started":"2023-06-10T21:23:00.411246Z","shell.execute_reply":"2023-06-10T21:23:00.418070Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:00.420502Z","iopub.execute_input":"2023-06-10T21:23:00.420985Z","iopub.status.idle":"2023-06-10T21:23:00.434869Z","shell.execute_reply.started":"2023-06-10T21:23:00.420960Z","shell.execute_reply":"2023-06-10T21:23:00.433921Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"The question data by applying the document_vector function to each question in the dataset. It creates two separate lists (X and X2) to store the document vectors for the questions in 'question1' and 'question2' columns, respectively.\n\nIt iterates over each question in the 'question1' column of the dataset. For each question, it calls the document_vector function to calculate the document vector and appends it to the X list. It performs a similar process for the 'question2' column and appends the document vectors to the X2 list. The X and X2 lists are converted into numpy arrays (X = np.array(X) and X2 = np.array(X2)). Two temporary data frames (temp_df1 and temp_df2) are created using the document vectors as the data and the original data frame index as the index. The temporary data frames are concatenated along the column axis to create a new data frame (temp_df). The 'id', 'qid1', 'qid2', 'question1', and 'question2' columns are dropped from the new_df data frame, and the result is stored in final_df. The shape of final_df is printed. The final_df and temp_df data frames are concatenated along the column axis to create the complete_df. The shape of complete_df is printed. The head() method is called on complete_df to display the first few rows.","metadata":{}},{"cell_type":"code","source":"X = []\nfor doc in tqdm(ques_df['question1']):\n    X.append(document_vector(doc))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:00.435945Z","iopub.execute_input":"2023-06-10T21:23:00.436185Z","iopub.status.idle":"2023-06-10T21:23:02.493286Z","shell.execute_reply.started":"2023-06-10T21:23:00.436162Z","shell.execute_reply":"2023-06-10T21:23:02.492428Z"},"trusted":true},"execution_count":117,"outputs":[{"name":"stderr","text":"100%|██████████| 30000/30000 [00:02<00:00, 14686.40it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"X2 = []\nfor doc in tqdm(ques_df['question2']):\n    X2.append(document_vector(doc))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:02.494529Z","iopub.execute_input":"2023-06-10T21:23:02.494860Z","iopub.status.idle":"2023-06-10T21:23:04.575081Z","shell.execute_reply.started":"2023-06-10T21:23:02.494832Z","shell.execute_reply":"2023-06-10T21:23:04.573533Z"},"trusted":true},"execution_count":118,"outputs":[{"name":"stderr","text":"100%|██████████| 30000/30000 [00:02<00:00, 14489.22it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"X = np.array(X)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:04.576688Z","iopub.execute_input":"2023-06-10T21:23:04.578044Z","iopub.status.idle":"2023-06-10T21:23:04.615666Z","shell.execute_reply.started":"2023-06-10T21:23:04.577994Z","shell.execute_reply":"2023-06-10T21:23:04.614048Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"X2 = np.array(X2)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:04.619623Z","iopub.execute_input":"2023-06-10T21:23:04.620004Z","iopub.status.idle":"2023-06-10T21:23:04.656125Z","shell.execute_reply.started":"2023-06-10T21:23:04.619973Z","shell.execute_reply":"2023-06-10T21:23:04.655083Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"temp_df1 = pd.DataFrame(X, index= new_df.index)\ntemp_df2 = pd.DataFrame(X2, index= new_df.index)\ntemp_df = pd.concat([temp_df1, temp_df2], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:04.659451Z","iopub.execute_input":"2023-06-10T21:23:04.659742Z","iopub.status.idle":"2023-06-10T21:23:04.683836Z","shell.execute_reply.started":"2023-06-10T21:23:04.659717Z","shell.execute_reply":"2023-06-10T21:23:04.682383Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"temp_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:04.685280Z","iopub.execute_input":"2023-06-10T21:23:04.685640Z","iopub.status.idle":"2023-06-10T21:23:04.692517Z","shell.execute_reply.started":"2023-06-10T21:23:04.685613Z","shell.execute_reply":"2023-06-10T21:23:04.691118Z"},"trusted":true},"execution_count":122,"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"(30000, 200)"},"metadata":{}}]},{"cell_type":"code","source":"final_df = new_df.drop(columns=['id','qid1','qid2','question1','question2'])\nprint(final_df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:04.693829Z","iopub.execute_input":"2023-06-10T21:23:04.694126Z","iopub.status.idle":"2023-06-10T21:23:04.709591Z","shell.execute_reply.started":"2023-06-10T21:23:04.694097Z","shell.execute_reply":"2023-06-10T21:23:04.708382Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"(30000, 23)\n","output_type":"stream"}]},{"cell_type":"code","source":"complete_df = pd.concat([final_df, temp_df], axis=1)\nprint(complete_df.shape)\ncomplete_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:04.710531Z","iopub.execute_input":"2023-06-10T21:23:04.710816Z","iopub.status.idle":"2023-06-10T21:23:04.753579Z","shell.execute_reply.started":"2023-06-10T21:23:04.710790Z","shell.execute_reply":"2023-06-10T21:23:04.752269Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stdout","text":"(30000, 223)\n","output_type":"stream"},{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"        is_duplicate  q1_len  q2_len  q1_num_words  q2_num_words  word_common  \\\n398782             1      75      76            13            13           12   \n115086             0      48      56            13            16            8   \n327711             0     104     119            28            21            4   \n367788             0      58     145            14            32            1   \n151235             0      34      49             5             9            3   \n\n        word_total  word_share   cwc_min   cwc_max  ...     90     91     92  \\\n398782          26        0.46  0.874989  0.874989  ...  0.227  0.083 -0.038   \n115086          24        0.33  0.666644  0.499988  ...  0.178  0.066 -0.018   \n327711          38        0.11  0.000000  0.000000  ...  0.230  0.119 -0.005   \n367788          34        0.03  0.000000  0.000000  ...  0.377  0.031  0.230   \n151235          13        0.23  0.749981  0.599988  ...  0.161  0.056  0.096   \n\n           93     94     95     96     97     98     99  \n398782 -0.041  0.324  0.211 -0.000 -0.137  0.067  0.012  \n115086  0.093  0.536 -0.011  0.252 -0.174  0.019 -0.097  \n327711  0.019  0.446  0.086 -0.163 -0.129  0.043 -0.057  \n367788  0.194  0.620 -0.241  0.244  0.391 -0.038  0.179  \n151235 -0.086  0.447  0.097 -0.096  0.034 -0.027 -0.036  \n\n[5 rows x 223 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>is_duplicate</th>\n      <th>q1_len</th>\n      <th>q2_len</th>\n      <th>q1_num_words</th>\n      <th>q2_num_words</th>\n      <th>word_common</th>\n      <th>word_total</th>\n      <th>word_share</th>\n      <th>cwc_min</th>\n      <th>cwc_max</th>\n      <th>...</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>398782</th>\n      <td>1</td>\n      <td>75</td>\n      <td>76</td>\n      <td>13</td>\n      <td>13</td>\n      <td>12</td>\n      <td>26</td>\n      <td>0.46</td>\n      <td>0.874989</td>\n      <td>0.874989</td>\n      <td>...</td>\n      <td>0.227</td>\n      <td>0.083</td>\n      <td>-0.038</td>\n      <td>-0.041</td>\n      <td>0.324</td>\n      <td>0.211</td>\n      <td>-0.000</td>\n      <td>-0.137</td>\n      <td>0.067</td>\n      <td>0.012</td>\n    </tr>\n    <tr>\n      <th>115086</th>\n      <td>0</td>\n      <td>48</td>\n      <td>56</td>\n      <td>13</td>\n      <td>16</td>\n      <td>8</td>\n      <td>24</td>\n      <td>0.33</td>\n      <td>0.666644</td>\n      <td>0.499988</td>\n      <td>...</td>\n      <td>0.178</td>\n      <td>0.066</td>\n      <td>-0.018</td>\n      <td>0.093</td>\n      <td>0.536</td>\n      <td>-0.011</td>\n      <td>0.252</td>\n      <td>-0.174</td>\n      <td>0.019</td>\n      <td>-0.097</td>\n    </tr>\n    <tr>\n      <th>327711</th>\n      <td>0</td>\n      <td>104</td>\n      <td>119</td>\n      <td>28</td>\n      <td>21</td>\n      <td>4</td>\n      <td>38</td>\n      <td>0.11</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.230</td>\n      <td>0.119</td>\n      <td>-0.005</td>\n      <td>0.019</td>\n      <td>0.446</td>\n      <td>0.086</td>\n      <td>-0.163</td>\n      <td>-0.129</td>\n      <td>0.043</td>\n      <td>-0.057</td>\n    </tr>\n    <tr>\n      <th>367788</th>\n      <td>0</td>\n      <td>58</td>\n      <td>145</td>\n      <td>14</td>\n      <td>32</td>\n      <td>1</td>\n      <td>34</td>\n      <td>0.03</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.377</td>\n      <td>0.031</td>\n      <td>0.230</td>\n      <td>0.194</td>\n      <td>0.620</td>\n      <td>-0.241</td>\n      <td>0.244</td>\n      <td>0.391</td>\n      <td>-0.038</td>\n      <td>0.179</td>\n    </tr>\n    <tr>\n      <th>151235</th>\n      <td>0</td>\n      <td>34</td>\n      <td>49</td>\n      <td>5</td>\n      <td>9</td>\n      <td>3</td>\n      <td>13</td>\n      <td>0.23</td>\n      <td>0.749981</td>\n      <td>0.599988</td>\n      <td>...</td>\n      <td>0.161</td>\n      <td>0.056</td>\n      <td>0.096</td>\n      <td>-0.086</td>\n      <td>0.447</td>\n      <td>0.097</td>\n      <td>-0.096</td>\n      <td>0.034</td>\n      <td>-0.027</td>\n      <td>-0.036</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 223 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"complete_df.columns[0:24]","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:04.754954Z","iopub.execute_input":"2023-06-10T21:23:04.755269Z","iopub.status.idle":"2023-06-10T21:23:04.763411Z","shell.execute_reply.started":"2023-06-10T21:23:04.755243Z","shell.execute_reply":"2023-06-10T21:23:04.762391Z"},"trusted":true},"execution_count":125,"outputs":[{"execution_count":125,"output_type":"execute_result","data":{"text/plain":"Index([        'is_duplicate',               'q1_len',               'q2_len',\n               'q1_num_words',         'q2_num_words',          'word_common',\n                 'word_total',           'word_share',              'cwc_min',\n                    'cwc_max',              'csc_min',              'csc_max',\n                    'ctc_min',              'ctc_max',         'last_word_eq',\n              'first_word_eq',         'abs_len_diff',             'mean_len',\n       'longest_substr_ratio',           'fuzz_ratio',   'fuzz_partial_ratio',\n           'token_sort_ratio',      'token_set_ratio',                      0],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"complete_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:04.764337Z","iopub.execute_input":"2023-06-10T21:23:04.764618Z","iopub.status.idle":"2023-06-10T21:23:04.787584Z","shell.execute_reply.started":"2023-06-10T21:23:04.764594Z","shell.execute_reply":"2023-06-10T21:23:04.786127Z"},"trusted":true},"execution_count":126,"outputs":[{"execution_count":126,"output_type":"execute_result","data":{"text/plain":"is_duplicate    0\nq1_len          0\nq2_len          0\nq1_num_words    0\nq2_num_words    0\n               ..\n95              0\n96              0\n97              0\n98              0\n99              0\nLength: 223, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_tr,X_te,y_tr,y_te = train_test_split(complete_df.iloc[:,1:],complete_df.iloc[:,0],test_size=0.2,random_state=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:04.789695Z","iopub.execute_input":"2023-06-10T21:23:04.790110Z","iopub.status.idle":"2023-06-10T21:23:04.861359Z","shell.execute_reply.started":"2023-06-10T21:23:04.790070Z","shell.execute_reply":"2023-06-10T21:23:04.860371Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"**To train an LSTM model for binary classification:**\n\nIt splits the data into training and test sets using train_test_split, where complete_df.iloc[:,1:].values represents the input features (X) and complete_df.iloc[:,0].values represents the target variable (y). It applies feature scaling using StandardScaler to standardize the input features. The scaler is fit on the training data and then applied to transform both the training and test data. The input data for the LSTM model is reshaped to have the shape (number of samples, number of time steps, number of features) using np.reshape. In this case, the number of features is set toThe LSTM model is defined using Sequential from Keras and consists of an LSTM layer with 128 units and a dense output layer with sigmoid activation. The model is compiled with the binary cross-entropy loss function, the Adam optimizer, and accuracy as the evaluation metric. The model is trained using the fit function, where X_train_lstm and y_train are the training data, epochs=10 specifies the number of training epochs, batch_size=64 determines the batch size, and validation_data is provided as (X_test_lstm, y_test) for validation during training. The training progress and performance metrics are stored in the lstm_hist variable.","metadata":{}},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(complete_df.iloc[:,1:].values,complete_df.iloc[:,0].values,test_size=0.2,random_state=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:04.862884Z","iopub.execute_input":"2023-06-10T21:23:04.863472Z","iopub.status.idle":"2023-06-10T21:23:04.945480Z","shell.execute_reply.started":"2023-06-10T21:23:04.863439Z","shell.execute_reply":"2023-06-10T21:23:04.944653Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_stdscaled = scaler.transform(X_train)\nX_test_stdscaled = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:04.946582Z","iopub.execute_input":"2023-06-10T21:23:04.946899Z","iopub.status.idle":"2023-06-10T21:23:05.001287Z","shell.execute_reply.started":"2023-06-10T21:23:04.946872Z","shell.execute_reply":"2023-06-10T21:23:05.000093Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Separate scalar values and arrays\nX_train_scalar = []\nX_train_array = []\n\nfor value in X_train:\n    if isinstance(value, int):\n        X_train_scalar.append(value)\n    elif isinstance(value, np.ndarray):\n        X_train_array.append(value)\n\n# Convert scalar values to numpy array\nX_train_scalar = np.array(X_train_scalar)\n\n# Convert arrays to numpy array\nX_train_array = np.array(X_train_array)\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# Fit and transform the training data (arrays)\nif X_train_array.shape[0] > 0:\n    X_train_array_stdscaled = np.vstack(X_train_array)\n    X_train_array_stdscaled = scaler.fit_transform(X_train_array_stdscaled)\nelse:\n    X_train_array_stdscaled = np.array([])\n\n# Concatenate scalar values with standardized arrays\nif X_train_array_stdscaled.shape[0] > 0:\n    X_train_stdscaled = np.concatenate((X_train_scalar.reshape(-1, 1), X_train_array_stdscaled), axis=1)\nelse:\n    X_train_stdscaled = X_train_scalar.reshape(-1, 1)\n\n# Transform the test data (scalar values only)\nX_test_stdscaled = scaler.transform(X_test)\n\n# Verify the shapes\nprint(\"X_train_stdscaled shape:\", X_train_stdscaled.shape)\nprint(\"X_test_stdscaled shape:\", X_test_stdscaled.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.002345Z","iopub.execute_input":"2023-06-10T21:23:05.002685Z","iopub.status.idle":"2023-06-10T21:23:05.184928Z","shell.execute_reply.started":"2023-06-10T21:23:05.002657Z","shell.execute_reply":"2023-06-10T21:23:05.183054Z"},"trusted":true},"execution_count":130,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[130], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Concatenate scalar values with standardized arrays\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_train_array_stdscaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 33\u001b[0m     X_train_stdscaled \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scalar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_array_stdscaled\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     X_train_stdscaled \u001b[38;5;241m=\u001b[39m X_train_scalar\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 0 and the array at index 1 has size 24000"],"ename":"ValueError","evalue":"all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 0 and the array at index 1 has size 24000","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# Fit and transform the training data\nX_train_stdscaled = scaler.fit_transform(X_train)\n\n# Transform the test data\nX_test_stdscaled = scaler.transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.185852Z","iopub.status.idle":"2023-06-10T21:23:05.186278Z","shell.execute_reply.started":"2023-06-10T21:23:05.186042Z","shell.execute_reply":"2023-06-10T21:23:05.186061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshape the input data for LSTM\nX_train_lstm = np.reshape(X_train_stdscaled, (X_train_stdscaled.shape[0], X_train_stdscaled.shape[1], 1))\nX_test_lstm = np.reshape(X_test_stdscaled, (X_test_stdscaled.shape[0], X_test_stdscaled.shape[1], 1))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.188011Z","iopub.status.idle":"2023-06-10T21:23:05.189162Z","shell.execute_reply.started":"2023-06-10T21:23:05.188936Z","shell.execute_reply":"2023-06-10T21:23:05.188959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the LSTM model\nlstm_model = Sequential()\nlstm_model.add(LSTM(128, input_shape=(X_train_lstm.shape[1], 1)))\nlstm_model.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nlstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.191161Z","iopub.status.idle":"2023-06-10T21:23:05.191705Z","shell.execute_reply.started":"2023-06-10T21:23:05.191456Z","shell.execute_reply":"2023-06-10T21:23:05.191482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the BiLSTM model with validation data\nlstm_hist = lstm_model.fit(X_train_lstm, y_train, epochs=20, batch_size=64, validation_data=(X_test_lstm, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.193114Z","iopub.status.idle":"2023-06-10T21:23:05.193466Z","shell.execute_reply.started":"2023-06-10T21:23:05.193313Z","shell.execute_reply":"2023-06-10T21:23:05.193329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot the training history\nplt.plot(lstm_hist.history['accuracy'])\nplt.plot(lstm_hist.history['val_accuracy'])\nplt.title('LSTM Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\nplt.plot(lstm_hist.history['loss'])\nplt.plot(lstm_hist.history['val_loss'])\nplt.title('LSTM Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.194334Z","iopub.status.idle":"2023-06-10T21:23:05.194630Z","shell.execute_reply.started":"2023-06-10T21:23:05.194495Z","shell.execute_reply":"2023-06-10T21:23:05.194508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results suggest that the LSTM model achieved moderate performance in terms of accuracy on both the training and validation sets. The model's performance on the validation set is slightly lower than its performance on the training set, indicating some degree of overfitting or a discrepancy between the training and validation data. Further analysis and adjustments may be necessary to improve the model's performance.","metadata":{}},{"cell_type":"code","source":"# Predict on the test data\ny_pred_prob = lstm_model.predict(X_test_lstm)\ny_pred_lstm = (y_pred_prob > 0.5).astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.195610Z","iopub.status.idle":"2023-06-10T21:23:05.195906Z","shell.execute_reply.started":"2023-06-10T21:23:05.195774Z","shell.execute_reply":"2023-06-10T21:23:05.195787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the predicted values to 1D array\ny_pred_lstm = np.squeeze(y_pred_lstm)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.196674Z","iopub.status.idle":"2023-06-10T21:23:05.196963Z","shell.execute_reply.started":"2023-06-10T21:23:05.196833Z","shell.execute_reply":"2023-06-10T21:23:05.196846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred_lstm)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.198576Z","iopub.status.idle":"2023-06-10T21:23:05.198932Z","shell.execute_reply.started":"2023-06-10T21:23:05.198774Z","shell.execute_reply":"2023-06-10T21:23:05.198792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the confusion matrix\ncm = confusion_matrix(y_test, y_pred_lstm)\nprint(\"Confusion Matrix:\")\nprint(cm)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.203369Z","iopub.status.idle":"2023-06-10T21:23:05.204065Z","shell.execute_reply.started":"2023-06-10T21:23:05.203848Z","shell.execute_reply":"2023-06-10T21:23:05.203871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a heatmap of the confusion matrix\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.205086Z","iopub.status.idle":"2023-06-10T21:23:05.206633Z","shell.execute_reply.started":"2023-06-10T21:23:05.206319Z","shell.execute_reply":"2023-06-10T21:23:05.206358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate precision and recall\nprecision = precision_score(y_test, y_pred_lstm)\nrecall = recall_score(y_test, y_pred_lstm)\n\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.208228Z","iopub.status.idle":"2023-06-10T21:23:05.208660Z","shell.execute_reply.started":"2023-06-10T21:23:05.208469Z","shell.execute_reply":"2023-06-10T21:23:05.208487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate classification report\nreport = classification_report(y_test, y_pred_lstm)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.209695Z","iopub.status.idle":"2023-06-10T21:23:05.210531Z","shell.execute_reply.started":"2023-06-10T21:23:05.210317Z","shell.execute_reply":"2023-06-10T21:23:05.210339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **BiLSTM Model:**","metadata":{}},{"cell_type":"code","source":"# Define the BiLSTM model\nbilstm_model = Sequential()\nbilstm_model.add(Bidirectional(LSTM(128), input_shape=(X_train_lstm.shape[1], 1)))\nbilstm_model.add(Dense(1, activation='sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.211639Z","iopub.status.idle":"2023-06-10T21:23:05.212551Z","shell.execute_reply.started":"2023-06-10T21:23:05.212327Z","shell.execute_reply":"2023-06-10T21:23:05.212348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the model\nbilstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the BiLSTM model\nhist = bilstm_model.fit(X_train_lstm, y_train, epochs=10, batch_size=64,validation_data=(X_test_lstm, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.214058Z","iopub.status.idle":"2023-06-10T21:23:05.214396Z","shell.execute_reply.started":"2023-06-10T21:23:05.214236Z","shell.execute_reply":"2023-06-10T21:23:05.214257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the training history\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('BiLSTM Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('BiLSTM Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.214976Z","iopub.status.idle":"2023-06-10T21:23:05.215275Z","shell.execute_reply.started":"2023-06-10T21:23:05.215116Z","shell.execute_reply":"2023-06-10T21:23:05.215129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on the test data\ny_pred_bilstm = bilstm_model.predict(X_test_lstm)\n\n# Convert the predicted values to 1D array\ny_pred_bilstm = np.squeeze(y_pred_bilstm)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.216230Z","iopub.status.idle":"2023-06-10T21:23:05.216539Z","shell.execute_reply.started":"2023-06-10T21:23:05.216390Z","shell.execute_reply":"2023-06-10T21:23:05.216406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert y_test to binary format\nthreshold = 0.5\ny_test_binary = np.array(y_test > threshold, dtype=int)\ny_pred_bilstm = np.array(y_pred_bilstm > threshold, dtype=int)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.218138Z","iopub.status.idle":"2023-06-10T21:23:05.218446Z","shell.execute_reply.started":"2023-06-10T21:23:05.218307Z","shell.execute_reply":"2023-06-10T21:23:05.218322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate accuracy\naccuracy = accuracy_score(y_test_binary, y_pred_bilstm)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.219933Z","iopub.status.idle":"2023-06-10T21:23:05.220239Z","shell.execute_reply.started":"2023-06-10T21:23:05.220069Z","shell.execute_reply":"2023-06-10T21:23:05.220082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the confusion matrix\ncm = confusion_matrix(y_test_binary, y_pred_bilstm)\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Create a heatmap of the confusion matrix\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.221109Z","iopub.status.idle":"2023-06-10T21:23:05.221400Z","shell.execute_reply.started":"2023-06-10T21:23:05.221264Z","shell.execute_reply":"2023-06-10T21:23:05.221277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate precision and recall\nprecision = precision_score(y_test, y_pred_bilstm)\nrecall = recall_score(y_test, y_pred_bilstm)\n\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.222374Z","iopub.status.idle":"2023-06-10T21:23:05.222653Z","shell.execute_reply.started":"2023-06-10T21:23:05.222518Z","shell.execute_reply":"2023-06-10T21:23:05.222534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate classification report\nreport = classification_report(y_test, y_pred_bilstm)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.223753Z","iopub.status.idle":"2023-06-10T21:23:05.224031Z","shell.execute_reply.started":"2023-06-10T21:23:05.223894Z","shell.execute_reply":"2023-06-10T21:23:05.223911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The improved results of the BiLSTM classifier can be attributed to the use of bidirectional LSTM, which allows the model to capture information from both past and future contexts. The bidirectional nature helps the model understand the sequential dependencies in the input data more effectively, leading to enhanced performance in the classification task.","metadata":{}},{"cell_type":"markdown","source":"# **Machine Learning Models: RandomForestClassifiers and XGBclassifier:**","metadata":{}},{"cell_type":"markdown","source":"# 1. Random forest Classifier:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nrf = RandomForestClassifier(n_estimators=250,criterion='gini',n_jobs=4)\nrf.fit(X_train_stdscaled,y_train)\n\n# Calculate train accuracy\ny_train_pred = rf.predict(X_train_stdscaled)\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\nprint(\"Train Accuracy:\", train_accuracy)\n\n# Calculate test accuracy\ny_test_pred = rf.predict(X_test_stdscaled)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint(\"Test Accuracy:\", test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.225284Z","iopub.status.idle":"2023-06-10T21:23:05.225588Z","shell.execute_reply.started":"2023-06-10T21:23:05.225453Z","shell.execute_reply":"2023-06-10T21:23:05.225469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the confusion matrix\ncm = confusion_matrix(y_test, y_test_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Create a heatmap of the confusion matrix\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.226724Z","iopub.status.idle":"2023-06-10T21:23:05.227008Z","shell.execute_reply.started":"2023-06-10T21:23:05.226866Z","shell.execute_reply":"2023-06-10T21:23:05.226882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate precision and recall\nprecision = precision_score(y_test, y_test_pred)\nrecall = recall_score(y_test, y_test_pred)\n\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.228011Z","iopub.status.idle":"2023-06-10T21:23:05.228330Z","shell.execute_reply.started":"2023-06-10T21:23:05.228150Z","shell.execute_reply":"2023-06-10T21:23:05.228166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate classification report\nreport = classification_report(y_test, y_test_pred)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.229268Z","iopub.status.idle":"2023-06-10T21:23:05.229543Z","shell.execute_reply.started":"2023-06-10T21:23:05.229408Z","shell.execute_reply":"2023-06-10T21:23:05.229423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overall, the Random Forest classifier achieves a relatively high test accuracy and captures a good number of true positives and true negatives. However, the possibility of overfitting should be considered due to the significantly higher training accuracy. Fine-tuning the model or exploring other algorithms might be necessary to further improve the performance and address any overfitting issues.","metadata":{}},{"cell_type":"markdown","source":"# 2. XGBoost Classifier:","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb = XGBClassifier(eta=0.1,n_estimators=200,n_jobs=4, learning_rate=0.1)\nxgb.fit(X_train_stdscaled,y_train)\n\n# Calculate train accuracy\ny_train_pred = xgb.predict(X_train_stdscaled)\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\nprint(\"Train Accuracy:\", train_accuracy)\n\n\n# Calculate test accuracy\ny_test_pred1 = xgb.predict(X_test_stdscaled)\ntest_accuracy = accuracy_score(y_test, y_test_pred1)\nprint(\"Test Accuracy:\", test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.230197Z","iopub.status.idle":"2023-06-10T21:23:05.230539Z","shell.execute_reply.started":"2023-06-10T21:23:05.230370Z","shell.execute_reply":"2023-06-10T21:23:05.230392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the confusion matrix\ncm = confusion_matrix(y_test, y_test_pred1)\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Create a heatmap of the confusion matrix\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.231437Z","iopub.status.idle":"2023-06-10T21:23:05.231733Z","shell.execute_reply.started":"2023-06-10T21:23:05.231587Z","shell.execute_reply":"2023-06-10T21:23:05.231603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate precision and recall\nprecision = precision_score(y_test, y_test_pred1)\nrecall = recall_score(y_test, y_test_pred1)\n\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.233458Z","iopub.status.idle":"2023-06-10T21:23:05.233776Z","shell.execute_reply.started":"2023-06-10T21:23:05.233602Z","shell.execute_reply":"2023-06-10T21:23:05.233621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate classification report\nreport = classification_report(y_test, y_test_pred1)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T21:23:05.234497Z","iopub.status.idle":"2023-06-10T21:23:05.234768Z","shell.execute_reply.started":"2023-06-10T21:23:05.234632Z","shell.execute_reply":"2023-06-10T21:23:05.234648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overall, the XGBoost classifier achieves a relatively high test accuracy and captures a good number of true positives and true negatives. It performs slightly better than the Random Forest classifier in terms of accuracy. However, similar to the Random Forest classifier, fine-tuning the model or exploring other algorithms might be necessary to further improve the performance and address any potential limitations.","metadata":{}}]}